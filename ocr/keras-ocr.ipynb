{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f2b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/codes/progress_til_beadas/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121142fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, keras_ocr,re, os\n",
    "from ocr.brands_list import brands\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efaa52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get distances for detections\n",
    "def get_distance(predictions):\n",
    "    x0, y0 = 0, 0\n",
    "    detections = []\n",
    "    for group in predictions:\n",
    "        top_left_x, top_left_y = group[1][0]\n",
    "        bottom_right_x = group[1][1][0]\n",
    "        bottom_right_y = group[1][3][1] # correct bottom-right coordinates\n",
    "        center_x = (top_left_x + bottom_right_x) / 2\n",
    "        center_y = (top_left_y + bottom_right_y) / 2\n",
    "        distance_from_origin = math.dist([x0, y0], [center_x, center_y])\n",
    "        distance_y = center_y - y0\n",
    "        detections.append({\n",
    "            \"text\": group[0],\n",
    "            \"center_x\": center_x,\n",
    "            \"center_y\": center_y,\n",
    "            \"distance_from_origin\": distance_from_origin,\n",
    "            \"distance_y\": distance_y,\n",
    "        })\n",
    "    return detections\n",
    "\n",
    "# Function to distinguish rows\n",
    "def distinguish_rows(lst, thresh=15):\n",
    "    sublists = []\n",
    "    for i in range(len(lst) - 1):\n",
    "        if lst[i + 1][\"distance_y\"] - lst[i][\"distance_y\"] <= thresh:\n",
    "            if lst[i] not in sublists:\n",
    "                sublists.append(lst[i])\n",
    "            sublists.append(lst[i + 1])\n",
    "        else:\n",
    "            yield sublists\n",
    "            sublists = [lst[i + 1]]\n",
    "    yield sublists\n",
    "    \n",
    "# Check for brand keywords\n",
    "def contains_expression(word_list, expressions):\n",
    "    result_list = []\n",
    "    for key in expressions:\n",
    "        key_lower = re.escape(key.lower())\n",
    "        # Check if the key exists as a complete word\n",
    "        if re.search(rf'\\b{key_lower}\\b', word_list):\n",
    "            result_list.append(key)\n",
    "        # Check if any of the associated values exist as complete words\n",
    "        for value in expressions[key]:\n",
    "            value_lower = re.escape(value.lower())\n",
    "            if re.search(rf'\\b{value_lower}\\b', word_list):\n",
    "                result_list.append(key)\n",
    "    if(not result_list):\n",
    "        return None\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82a066d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:24:31.484580Z",
     "iopub.status.busy": "2024-07-10T15:24:31.483706Z",
     "iopub.status.idle": "2024-07-10T15:25:15.410579Z",
     "shell.execute_reply": "2024-07-10T15:25:15.409256Z"
    },
    "papermill": {
     "duration": 44.080847,
     "end_time": "2024-07-10T15:25:15.412307",
     "exception": true,
     "start_time": "2024-07-10T15:24:31.331460",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_ocr_pipeline():\n",
    "    \"\"\"Initialize and return keras-ocr pipeline\"\"\"\n",
    "    print(\"[DEBUG] Initializing keras-ocr pipeline\")\n",
    "    return keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "def read_image_safely(image_path):\n",
    "    \"\"\"Read image with error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"[DEBUG] Attempting to read image: {image_path}\")\n",
    "        read_image = keras_ocr.tools.read(image_path)\n",
    "        if read_image is None or read_image.size == 0:\n",
    "            raise ValueError(\"Image is empty or None\")\n",
    "        print(\"[DEBUG] Successfully read image\")\n",
    "        return read_image\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Failed to read image: {image_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_brand_with_llm(text, brand, api_key):\n",
    "    \"\"\"Check ambiguous brand mentions using LLM\"\"\"\n",
    "    print(f\"[DEBUG] Checking ambiguous brand '{brand}' with LLM\")\n",
    "    system_message = f\"you decide if the sentence is about {brand} as a brand or the context says otherwise. Your answers can be: 'yes, no, cannot decide'\"\n",
    "    \n",
    "    client = Groq(api_key=api_key)\n",
    "    print(\"[DEBUG] Making LLM API call\")\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "    \n",
    "    response_content = chat_completion.choices[0].message.content\n",
    "    print(f\"[DEBUG] LLM response: {response_content}\")\n",
    "    return \"yes\" in response_content.lower()\n",
    "\n",
    "def process_single_image(image_path, pipeline, brands, brands_dict, api_key, ambiguous_brands):\n",
    "    \"\"\"Process a single image and update brands dictionary\"\"\"\n",
    "    print(f\"\\n[DEBUG] Processing image: {image_path}\")\n",
    "    \n",
    "    # Read image\n",
    "    read_image = read_image_safely(image_path)\n",
    "    if read_image is None:\n",
    "        print(\"[DEBUG] Skipping image due to read failure\")\n",
    "        return brands_dict\n",
    "        \n",
    "    # Get predictions\n",
    "    print(\"[DEBUG] Running OCR prediction\")\n",
    "    prediction_groups = pipeline.recognize([read_image])\n",
    "    predictions = get_distance(prediction_groups[0])\n",
    "    print(f\"[DEBUG] Raw predictions: {predictions}\")\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions = distinguish_rows(predictions)\n",
    "    if predictions is None:\n",
    "        predictions = []\n",
    "    \n",
    "    predictions = list(filter(lambda x: x != [], predictions))\n",
    "    ordered_preds = [each[\"text\"] for row in predictions for each in sorted(row, key=lambda x: x[\"distance_from_origin\"])]\n",
    "    ordered_text = \" \".join(ordered_preds).lower()\n",
    "    print(f\"[DEBUG] Processed text: {ordered_text}\")\n",
    "\n",
    "    # Check for brands\n",
    "    result_list = contains_expression(ordered_text, brands)\n",
    "    print(f\"[DEBUG] Detected brands: {result_list}\")\n",
    "\n",
    "    if result_list:\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        for brand in result_list:\n",
    "            print(f\"[DEBUG] Processing brand: {brand}\")\n",
    "            if brand in ambiguous_brands:\n",
    "                print(f\"[DEBUG] {brand} is ambiguous, checking with LLM\")\n",
    "                if check_brand_with_llm(ordered_text, brand, api_key):\n",
    "                    if filename not in brands_dict[brand]:\n",
    "                        print(f\"[DEBUG] Adding {filename} to {brand}\")\n",
    "                        brands_dict[brand].append(filename)\n",
    "            else:\n",
    "                if brand is not None and filename not in brands_dict[brand]:\n",
    "                    print(f\"[DEBUG] Adding {filename} to {brand}\")\n",
    "                    brands_dict[brand].append(filename)\n",
    "                    \n",
    "    return brands_dict\n",
    "\n",
    "def process_images_in_directory(directory, brands, api_key, ambiguous_brands):\n",
    "    \"\"\"Process all images in directory and detect brands\"\"\"\n",
    "    print(f\"\\n[DEBUG] Starting directory processing: {directory}\")\n",
    "    # Initialize OCR pipeline once\n",
    "    pipeline = initialize_ocr_pipeline()\n",
    "    brands_dict = {brand: [] for brand in brands}\n",
    "    print(f\"[DEBUG] Initialized brands dictionary with {len(brands)} brands\")\n",
    "\n",
    "    # Process files directly in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            print(f\"\\n[DEBUG] Processing file: {filename}\")\n",
    "            brands_dict = process_single_image(\n",
    "                image_path,\n",
    "                pipeline,\n",
    "                brands,\n",
    "                brands_dict,\n",
    "                api_key,\n",
    "                ambiguous_brands\n",
    "            )\n",
    "    \n",
    "    print(\"[DEBUG] Finished processing all images\")\n",
    "    return brands_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46daa4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Starting brand detection with ambiguous brands: ['apple', 'oracle', 'amazon', 'tesla', 'oracle', 'visa', 'zara', 'ge', 'ford', 'corona', 'intel', 'linkedin', 'hp', 'hermes']\n",
      "\n",
      "[DEBUG] Starting directory processing: ocr/brands_images\n",
      "[DEBUG] Initializing keras-ocr pipeline\n",
      "Looking for /Users/beszabo/.keras-ocr/craft_mlt_25k.h5\n",
      "WARNING:tensorflow:From /Users/beszabo/codes/progress_til_beadas/.venv/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n",
      "Looking for /Users/beszabo/.keras-ocr/crnn_kurapan.h5\n",
      "[DEBUG] Initialized brands dictionary with 100 brands\n",
      "\n",
      "[DEBUG] Processing file: .DS_Store\n",
      "\n",
      "[DEBUG] Processing image: ocr/brands_images/.DS_Store\n",
      "[DEBUG] Attempting to read image: ocr/brands_images/.DS_Store\n",
      "[DEBUG] Failed to read image: ocr/brands_images/.DS_Store. Error: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "[DEBUG] Skipping image due to read failure\n",
      "\n",
      "[DEBUG] Processing file: airbnb_example.jpg\n",
      "\n",
      "[DEBUG] Processing image: ocr/brands_images/airbnb_example.jpg\n",
      "[DEBUG] Attempting to read image: ocr/brands_images/airbnb_example.jpg\n",
      "[DEBUG] Successfully read image\n",
      "[DEBUG] Running OCR prediction\n",
      "1/1 [==============================] - 1s 629ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/codes/progress_til_beadas/.venv/lib/python3.11/site-packages/shapely/constructive.py:1353: RuntimeWarning: divide by zero encountered in oriented_envelope\n",
      "  return lib.oriented_envelope(geometry, **kwargs)\n",
      "/Users/beszabo/codes/progress_til_beadas/.venv/lib/python3.11/site-packages/shapely/constructive.py:1353: RuntimeWarning: invalid value encountered in oriented_envelope\n",
      "  return lib.oriented_envelope(geometry, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 899ms/step\n",
      "[DEBUG] Raw predictions: [{'text': 'lq', 'center_x': 105.0, 'center_y': 97.0, 'distance_from_origin': 142.9475428260311, 'distance_y': 97.0}, {'text': 'airbnb', 'center_x': 118.99998474121094, 'center_y': 189.99998474121094, 'distance_from_origin': 224.1896308263806, 'distance_y': 189.99998474121094}]\n",
      "[DEBUG] Processed text: airbnb\n",
      "[DEBUG] Detected brands: ['airbnb']\n",
      "[DEBUG] Processing brand: airbnb\n",
      "[DEBUG] Adding airbnb_example.jpg to airbnb\n",
      "[DEBUG] Finished processing all images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "ambiguous_brands = [\"apple\", \"oracle\", \"amazon\", \"tesla\", \"oracle\", \"visa\", \"zara\", \"ge\", \"ford\", \"corona\", \"intel\", \"linkedin\", \"hp\", \"hermes\"]\n",
    "print(\"[DEBUG] Starting brand detection with ambiguous brands:\", ambiguous_brands)\n",
    "results = process_images_in_directory(\n",
    "    Path('ocr/brands_images'), \n",
    "    brands, \n",
    "    GROQ_API_KEY, \n",
    "    ambiguous_brands\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589ea50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airbnb: 1\n"
     ]
    }
   ],
   "source": [
    "non_empty_brands_dict = {key: value for key, value in results.items() if value}\n",
    "\n",
    "for key, value in non_empty_brands_dict.items():\n",
    "    print(f\"{key}: {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fb9221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T14:12:42.765557Z",
     "iopub.status.busy": "2024-07-10T14:12:42.765174Z",
     "iopub.status.idle": "2024-07-10T14:12:47.238938Z",
     "shell.execute_reply": "2024-07-10T14:12:47.237323Z",
     "shell.execute_reply.started": "2024-07-10T14:12:42.765527Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c906b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 20672,
     "sourceId": 28131,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5206346,
     "sourceId": 8921522,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5366753,
     "sourceId": 8922601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 292.174707,
   "end_time": "2024-07-10T15:25:17.691562",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T15:20:25.516855",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
